<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DORA JAMBOR</title>
    <link>https://dorajam.github.io/thoughts/</link>
    <description>Recent content on DORA JAMBOR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Sep 2016 18:27:39 +0200</lastBuildDate>
    <atom:link href="https://dorajam.github.io/thoughts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An intuition behind eigenvalues</title>
      <link>https://dorajam.github.io/thoughts/post/eigenvalues/</link>
      <pubDate>Fri, 23 Sep 2016 18:27:39 +0200</pubDate>
      
      <guid>https://dorajam.github.io/thoughts/post/eigenvalues/</guid>
      <description>&lt;p&gt;This article aims to give a thorough intuition behind the significance of eigenvectors.&lt;/p&gt;

&lt;p&gt;The first time I got introduced to eigenvectors and eigenvalues, I thought ‘wow this is easy, what’s all the fuss about’. Yet further down the line upon discovering that these concepts are actually used in some very complex systems, I started losing my confidence. There was one question I just could not answer to myself:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do eigenvalues matter so much? And how can they be applied to such a wide variety of areas?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Generally, I find that there is a huge gap between familiarising oneself with the math of linear algebra and actually developing a visual intuition behind core concepts. Due to the lack of focus on the latter, this aspect of the field only tends to develop over a long course of time. This should not be this way though.&lt;/p&gt;

&lt;p&gt;Recognising this for myself has made me develop quite a strong love for the field. With this post, I will attempt to give you a fun introduction to the significance of eigenvalues and thereby hope you’ll start seeing the bright light at the end of the tunnel of all those dry mathematical derivations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An introduction to eigenvectors and eigenvalues&lt;/strong&gt;
And here we begin. You have probably stumbled upon the definition of eigenvalues:
$$
\begin{align}
AV=\lambda v
\end{align}
$$&lt;/p&gt;

&lt;p&gt;Where &lt;code&gt;$v$&lt;/code&gt; is an eigenvector and &lt;code&gt;$\lambda $&lt;/code&gt; is the corresponding eigenvalue. Let’s look at the right side first: You have a vector &lt;code&gt;$v$&lt;/code&gt; that you apply the linear transformation &lt;code&gt;$A$&lt;/code&gt;. On the left: you have the same &lt;code&gt;$v$&lt;/code&gt; vector, but now scaled by some scalar value.&lt;/p&gt;

&lt;p&gt;So you can think of eigenvectors as vectors that do not change their direction after some linear transformation, but merely get scaled by some value. (well, they get reversed if the eigenvalue is negative).&lt;/p&gt;

&lt;p&gt;Now going back to the world of linear transformation, you can think of &lt;code&gt;$A$&lt;/code&gt; as some linear transformation and &lt;code&gt;$v$&lt;/code&gt; as a vector that’s a linear combination of the bases of the space it is defined in (let’s say v = alpha*i + beta*j where alpha and beta are members of the component space). When applying A linear transformation to v, you are actually transforming the whole vector space v was defined in. But hey v was defined in terms of the bases of that space (e.g. i and j), so the bases of the new vector ‘Av&amp;rsquo; will be some new i&amp;rsquo; and j’ basis vectors. This is where it is extremely important that you understand what’s going on visually.&lt;/p&gt;

&lt;p&gt;If you have difficulty internalising this, I &lt;strong&gt;very much&lt;/strong&gt; recommend you read my previous post on &lt;a href=&#34;https://dorajam.github.io/thoughts/post/change-of-basis/&#34;&gt;changing the basis&lt;/a&gt; of some vector space.&lt;/p&gt;

&lt;p&gt;With this in mind, we can think of eigenvectors as vectors that preserve their directions in this new transformed universe. What’s so fascinating about this is that we can transform any vector from the original vector space into this new space just by knowing what the transformation’s eigenpairs are. Doing this will allow us to reduce massive amounts of information that’s needed to reconstruct all points in this new, transformed universe!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s more, we can think about eigenvalues in terms of polynomials!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The concept of roots is important as they serve as some fixed ground to a polynomial. Upon finding all roots to a function, we have some notion of the function shape. The same way, we can think of eigenvectors as some components of a vector space that limit the behaviour of a linear transformation.&lt;/p&gt;

&lt;p&gt;Good news, you don’t need to blindly trust me on this one.
Remember equation &lt;code&gt;$(1)$&lt;/code&gt;, we can derive the eigenpairs by finding the null space of the determinant of A - /\I (i.e. det(A-/\I) = 0). If A is R(nxn) then the determinant of the latter term will result in a polynomial of degree n. By the fundamental theorem of algebra, a polynomial of degree n has n (not necessarily unique) complex roots. These are the eigenvalues of A. We can think about these values as &lt;em&gt;”how much should linear transformation A be distorted so that we’ll end up in null space?”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Why do we care about ending up in null space? Because this is the place where things go extinct. There is no way of reconstructing the previous vector space if all dimensions cease to exist. Think about matrix inversion! You could not flip space back into its original state as you just lost all your information about where you were (with no notation of basis!)&lt;/p&gt;

&lt;p&gt;Fascinating, isn’t it?&lt;/p&gt;

&lt;p&gt;To summarise this point, roughly you could say that the “degree&amp;rdquo; and the orientation of a distortion induced by the transformation can be measured by eigenvalues and eigenvectors, respectively.&lt;/p&gt;

&lt;p&gt;If this wasn’t fascinating enough, allow me to keep going.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dimensionality Reduction and eigenbases&lt;/strong&gt;
You may have heard that this can all be applied to &lt;strong&gt;dimensionality reduction&lt;/strong&gt;. How so? Well, let’s see.
Eigenvectors are linearly independent. That means, that no eigenvector can be reconstructed by taking a linear combination of another eigenvector: we just simply need them all! This might already make you ponder, would it be possible to switch from using our standard basis vectors to using these powerful eigenvectors that share this special quality? Well it depends.&lt;/p&gt;

&lt;p&gt;There are three scenarios when decomposing eigenvectors:
&lt;strong&gt;(i)&lt;/strong&gt; &lt;em&gt;no eigenvectors exist&lt;/em&gt;
&lt;strong&gt;(ii)&lt;/strong&gt; _the number of eigenvectors is less than the number of dimensions__
&lt;strong&gt;(iii)&lt;/strong&gt; &lt;em&gt;the number of eigenvectors is the same as the number of dimensions&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Change of basis in a vector space</title>
      <link>https://dorajam.github.io/thoughts/post/change-of-basis/</link>
      <pubDate>Tue, 20 Sep 2016 21:53:36 +0200</pubDate>
      
      <guid>https://dorajam.github.io/thoughts/post/change-of-basis/</guid>
      <description>&lt;p&gt;A fascinating characteristic of the human mind is the ability to make connections between different objects and phenomena. What allows us to do this is a common ground along which things can be understood, connected and thus compared.&lt;/p&gt;

&lt;p&gt;You can think of units as the words and expressions that define a language. I say ‘apple’ in my language, which you may or may not perceive as the same object depending whether you speak the same language as me.&lt;/p&gt;

&lt;p&gt;To represent different points in space we need some sort of unit of measurement that allows us to speak about things in the same language. Space has no intrinsic grid. Our chosen units will allow us to understand space by breaking it into some imaginary gridlines. Along these gridlines we can define any point by using our unit of basis. This basis serves as some kind of constructing element or brick that makes up the whole.&lt;/p&gt;

&lt;p&gt;Now imagine a flat world, a two-dimensional universe or plane that’s defined along x and y axes. The conventional way of representing points in 2D space is to use the standard coordinate system, defined by the basis vectors (i) and (j). Notice, that it is just a choice we have collectively made. We could also represent points in 2D using some alternative units of measurement (i.e. along some different set of gridlines).
Given our choice, we can now think of a vector as a scaled version of our basis vectors (i) and (j):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$$
  v=\begin{pmatrix}
  3\\
  2
 \end{pmatrix}
  *\begin{pmatrix}
  1 &amp;amp; 0 \\
  0 &amp;amp; 1
 \end{pmatrix}
$$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As in our standard system we always refer to &lt;code&gt;\(i\)&lt;/code&gt; and &lt;code&gt;\(j\)&lt;/code&gt; as our basis, we can ignore them and just use the following notation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$$
  v=\begin{pmatrix}
  3\\
  2
 \end{pmatrix}
$$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Some thoughts on linear transformation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When it comes to matrix-vector multiplication, we can think of the matrix in terms of some linear transformation that will redefine the bases of the coordinate system. Applying some transformation (A) to vector (v) from above:&lt;/p&gt;

&lt;p&gt;[
 Av=\begin{pmatrix}
  1 &amp;amp; 2 \
  4 &amp;amp; 3
 \end{pmatrix}*
  \begin{pmatrix}
  3\
  2
 \end{pmatrix}
]&lt;/p&gt;

&lt;p&gt;is actually equivalent to scaling the column vectors of (A) by the entries of (v):&lt;/p&gt;

&lt;p&gt;[
 Av=2\begin{pmatrix}
  1 \
  2
 \end{pmatrix}+
  3\begin{pmatrix}
  4 \
  3
 \end{pmatrix}
]&lt;/p&gt;

&lt;p&gt;This way we can translate the above operation into a set of new basis vectors defined by (A) and a component space that will determine how much we are stretching each dimension of the system we are currently in.&lt;/p&gt;

&lt;p&gt;Putting this into new light, we can think of this operation as preserving the component space ((v)) of our standard coordinate system, but &lt;em&gt;adjust our units according to the transformation matrix&lt;/em&gt;. As a result, this new vector can now be understood in terms of a new set of basis vectors.&lt;/p&gt;

&lt;p&gt;But hey, not so fast! As long as we think of the new bases in terms of some scaled version of (i) and (j), we are only speaking in our own language. We merely found a translation of a vector from an alternative universe into our system.&lt;/p&gt;

&lt;p&gt;This is illustrated beautifully by the &lt;a href=&#34;https://www.youtube.com/watch?v=P2LTAUO1TdA&#34;&gt;3Blue1Brown guy&lt;/a&gt; on Youtube.&lt;/p&gt;

&lt;p&gt;The reason I emphasised those imaginary gridlines drawn by our chosen units of basis earlier was to put you into perspective on which system of bases we are referring to. To understand this new vector in terms of your new bases, you would want to think your bases as your new units of measurement. That is, you would like to teleport from thinking of (\begin{pmatrix}1 \ 4\end{pmatrix}) as (\begin{pmatrix}1 \ 0\end{pmatrix}) and of (\begin{pmatrix}2 \ 3\end{pmatrix}) as (\begin{pmatrix}0 \ 1\end{pmatrix}). I recommend you try to visualise this in your head - or have a look at &lt;a href=&#34;https://www.youtube.com/watch?v=kYB8IZa5AuE&#34;&gt;this&lt;/a&gt;.
We can think of some linear operation as stretching, compressing, rotating, flipping our space, or well.. adjusting those gridlines along which we can refer to points in space.&lt;/p&gt;

&lt;p&gt;We are almost there, yay! By now, you may already be pondering about a couple of things…
((i)) &lt;em&gt;How is a point in space represented in one system compared to another?&lt;/em&gt;
((ii)) &lt;em&gt;And how do we understand the same linear transformation in another system?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let me jump on the first one.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As stated earlier, a vector is just a scaled version of its bases. Remember, it is really just the product of a matrix (whose columns are the bases) and a vector whose entries are the components). What we did earlier was preserving the component space, but transforming our system of bases. This ended up taking us to a new point in space. So how do we make sure to stay in the same point? Well, we need to apply some different transformation to our new basis vectors.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>